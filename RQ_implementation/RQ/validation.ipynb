{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def norm_min_max(df: pd.DataFrame, col: str):\n",
    "    values = df[col]\n",
    "    return (values - values.min()) / (values.max() - values.min())\n",
    "\n",
    "\n",
    "def load_task_and_preprocess(results_file, task_name):\n",
    "\n",
    "    with open(results_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    list_json = []\n",
    "    for l in lines:\n",
    "        list_json.append(json.loads(l))\n",
    "    df = pd.DataFrame(list_json)\n",
    "\n",
    "    acc_keys = {\"livecodebench\": \"acc\", \"code2text_python\": \"smoothed_bleu_4,create_output\"}\n",
    "    df = df[df[\"task_name\"] == task_name]\n",
    "    df[\"model\"] = df[\"model\"].apply(lambda x: x.split(\"/\")[1])\n",
    "    df[\"params\"] = df[\"model\"].apply(lambda x: re.findall(r\"(\\d+(?:\\.\\d+)?[bBmM])\", x.upper())[0])\n",
    "    df[\"acc_values\"] = df[\"acc_values\"].apply(lambda x: x[acc_keys[task_name]])\n",
    "    df = df.reset_index()\n",
    "    df = df[[\"model\", \"params\", \"task_name\", \"acc_values\", \"energy_consumed\"]]\n",
    "    return df\n",
    "\n",
    "df_lcb = load_task_and_preprocess(\"../../lm_eval/results/final_results.jsonl\", \"livecodebench\")\n",
    "df_c2t = load_task_and_preprocess(\"../../lm_eval/results/final_results.jsonl\", \"code2text_python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcb = df_lcb.drop(columns=[\"task_name\"])\n",
    "df_c2t = df_c2t.drop(columns=[\"task_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    df[\"energy_norm\"] = norm_min_max(df, \"energy_consumed\")\n",
    "    df[\"ene_eff\"] = 1 - df[\"energy_norm\"]\n",
    "    df[\"perf\"] = norm_min_max(df, \"acc_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ae32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(df_lcb)\n",
    "normalize(df_c2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bac5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm, LinearSegmentedColormap\n",
    "\n",
    "\n",
    "center = (1, 1)\n",
    "\n",
    "def calculate_euc_formula(df):\n",
    "    return ((1 - df[\"perf\"]) ** 2 + (1 - df[\"ene_eff\"]) ** 2) ** 0.5\n",
    "    \n",
    "def fill_distance_ranking(df, distance):\n",
    "    df = df.copy()\n",
    "    df[\"CIRC_rank\"] = 0\n",
    "    # Circle parameters\n",
    "    radiuses = np.linspace(0, np.sqrt(2), 6)\n",
    "    selected_so_far = set()\n",
    "    curr_rank = 5\n",
    "    for r in radiuses:\n",
    "        if r == 0:\n",
    "            continue\n",
    "        less_than_r = df[distance < r].index.to_list()\n",
    "        new_points = set(less_than_r).difference(selected_so_far)\n",
    "        selected_so_far.update(less_than_r)\n",
    "        if new_points:\n",
    "            df.loc[list(new_points), \"CIRC_rank\"] = curr_rank\n",
    "        else:\n",
    "            pass\n",
    "        curr_rank -= 1\n",
    "    return df[\"CIRC_rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_lcb = calculate_euc_formula(df_lcb)\n",
    "distance_c2t = calculate_euc_formula(df_c2t)\n",
    "\n",
    "circ_lcb_ranks = fill_distance_ranking(df_lcb, distance_lcb)\n",
    "circ_c2t_ranks = fill_distance_ranking(df_c2t, distance_c2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcb[\"CIRC_rank\"] = circ_lcb_ranks\n",
    "df_c2t[\"CIRC_rank\"] = circ_c2t_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def remove_outliers(df, percentile=0.95):\n",
    "\n",
    "    X = df[[\"ene_eff\", \"perf\"]].to_numpy()\n",
    "    mcd = MinCovDet().fit(X)\n",
    "    D2 = mcd.mahalanobis(X)\n",
    "    cut = chi2.ppf(percentile, df=2)\n",
    "\n",
    "    outliers = D2 > cut\n",
    "    inliers = ~outliers\n",
    "    X_clean = X[inliers]\n",
    "    return X_clean\n",
    "\n",
    "def create_all_possible_derivatives(ene_eff, acc):\n",
    "    derivatives = []\n",
    "    for i in range(len(ene_eff)):\n",
    "        i_x = ene_eff.iloc[i]\n",
    "        i_y = acc.iloc[i]\n",
    "        for j in range(i+1, len(ene_eff)):\n",
    "            new_x = ene_eff.iloc[j]\n",
    "            new_y = acc.iloc[j]\n",
    "            if (new_x < i_x and new_y > i_y) or (new_x > i_x and new_y < i_y):\n",
    "                derivatives.append(-abs((new_y - i_y) / (new_x - i_x)))\n",
    "    return derivatives\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def remove_derivative_outliers(all_possible_derivates):\n",
    "\n",
    "    deriv = np.array(all_possible_derivates)\n",
    "    mcd = MinCovDet().fit(deriv.reshape(-1, 1))\n",
    "    # squared Mahalanobis distances under robust location/covariance:\n",
    "    d2 = mcd.mahalanobis(deriv.reshape(-1, 1))\n",
    "\n",
    "    thr = chi2.ppf(0.95, df=1)\n",
    "\n",
    "    deriv_inliers_all = deriv[d2 <= thr]\n",
    "    deriv_outliers_all = deriv[d2 > thr]\n",
    "    return deriv_inliers_all\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def approximate_regression_function(df, X_clean, deriv_inliers_all, quantile=75, degree=5, intercept=1e-2):\n",
    "\n",
    "    x_raw, y = X_clean[:, 0], X_clean[:, 1]\n",
    "\n",
    "\n",
    "    d = degree\n",
    "    b = cp.Variable(d + 1)\n",
    "\n",
    "    # Least-squares objective\n",
    "    x_transformed = np.vander(x_raw, N=d + 1, increasing=True)\n",
    "    objective = cp.Minimize(cp.sum_squares(x_transformed @ b - y))\n",
    "\n",
    "    # Enforce f′(z) ≤ 0 on a grid\n",
    "    z = np.linspace(0, 1, 50)\n",
    "    D = np.zeros((len(z), d + 1))\n",
    "    for j, zj in enumerate(z):\n",
    "        for k in range(1, d + 1):\n",
    "            D[j, k] = k * zj ** (k - 1)\n",
    "        \n",
    "    prob = cp.Problem(objective, [D @ b <= np.percentile(deriv_inliers_all, quantile), cp.sum(b) >= intercept])\n",
    "    prob.solve()\n",
    "    # X_plot = np.linspace(-0.1, 1.1, 100).reshape(-1, 1)\n",
    "    # X_grid = np.vander(X_plot.flatten(), N=d + 1, increasing=True)\n",
    "    # y_grid = X_grid @ b.value\n",
    "\n",
    "    # plt.figure(figsize=(5, 3))\n",
    "    # plt.scatter(X_clean[:, 0], X_clean[:, 1], s=30)\n",
    "    # plt.scatter(df[\"ene_eff\"], df[\"perf\"], c=\"red\", marker=\"x\", s=8)\n",
    "    # plt.plot(X_plot, y_grid, label=\"monotone ↓\")\n",
    "    # plt.xlabel(\"x\")\n",
    "    # plt.ylabel(\"y\")\n",
    "    # plt.title(\"Smooth Monotonic-Decreasing Regression\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    return b, d\n",
    "\n",
    "def regression_rank(df, b, d):\n",
    "    predicted_perf = np.vander(df[\"ene_eff\"], N=d + 1, increasing=True) @ b.value\n",
    "    score = df[\"perf\"] / predicted_perf\n",
    "    min_score, max_score = score.min(), score.max()\n",
    "    five_intervals = (max_score - min_score) / 5\n",
    "    oter_rank = np.ceil((score - min_score) / five_intervals)\n",
    "    oter_rank[oter_rank == 0] = 1\n",
    "    oter_rank = oter_rank.astype(int)\n",
    "    # tmp_df = pd.DataFrame()\n",
    "    # tmp_df[\"predicted\"] = predicted_perf\n",
    "    # tmp_df[\"perf\"] = df[\"perf\"].values\n",
    "    # tmp_df[\"score\"] = score.values\n",
    "    # tmp_df[\"OTER_rank\"] = oter_rank.values\n",
    "    # print(tmp_df)\n",
    "\n",
    "    return oter_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean_lcb = remove_outliers(df_lcb)\n",
    "X_clean_c2t = remove_outliers(df_c2t)\n",
    "all_possible_derivates_lcb = create_all_possible_derivatives(df_lcb[\"ene_eff\"], df_lcb[\"perf\"])\n",
    "all_possible_derivates_c2t = create_all_possible_derivatives(df_c2t[\"ene_eff\"], df_c2t[\"perf\"])\n",
    "deriv_inliers_all_lcb = remove_derivative_outliers(all_possible_derivates_lcb)\n",
    "deriv_inliers_all_c2t = remove_derivative_outliers(all_possible_derivates_c2t)\n",
    "coefficients_lcb, degree_lcb = approximate_regression_function(df_lcb, X_clean_lcb, deriv_inliers_all_lcb)\n",
    "coefficients_c2t, degree_c2t = approximate_regression_function(df_c2t, X_clean_c2t, deriv_inliers_all_c2t)\n",
    "oter_rank_lcb = regression_rank(df_lcb, coefficients_lcb, degree_lcb)\n",
    "oter_rank_c2t = regression_rank(df_c2t, coefficients_c2t, degree_c2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a873cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcb[\"OTER_rank\"] = oter_rank_lcb\n",
    "df_c2t[\"OTER_rank\"] = oter_rank_c2t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690de25",
   "metadata": {},
   "source": [
    "## Parameter not showing correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef25aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    if 3 <= x < 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "for i, df in {\"lcb\": df_lcb, \"cxg\": df_c2t}.items():\n",
    "    for method in [\"CIRC\", \"OTER\"]:\n",
    "        new_df = df.copy()\n",
    "        new_df[\"params\"] = new_df[\"params\"].apply(lambda x: float(x.replace(\"B\", \"\")) if \"B\" in x else float(x.replace(\"M\", \"\")) / 100).apply(categorize)\n",
    "\n",
    "        # assume df has columns: 'model_size' (0,1,2) and 'score'\n",
    "        groups = [new_df[new_df['params'] == g][f'{method}_rank'] for g in new_df['params'].unique()]\n",
    "        stat, p = kruskal(*groups)\n",
    "        print(f\"{i}-{method} Kruskal-Wallis H-statistic: {stat:.4f}, p-value: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21dc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"params\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b48e43",
   "metadata": {},
   "source": [
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dedda",
   "metadata": {},
   "source": [
    "# Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "for task, df in {\"lcb\": df_lcb, \"cxg\": df_c2t}.items():\n",
    "    base = \"CIRC_rank\"\n",
    "    for i in range(len(df)):\n",
    "        new_df = df.drop(df.index[i]).copy()     # safer than concat slices\n",
    "        normalize(new_df)                         # must operate on new_df only\n",
    "        dist = calculate_euc_formula(new_df)\n",
    "        circ_ranks = fill_distance_ranking(new_df, dist)  # index = new_df.index\n",
    "\n",
    "        col = f\"_leftcirc_{i}\"\n",
    "        df[col] = np.nan\n",
    "        df.loc[circ_ranks.index, col] = circ_ranks\n",
    "\n",
    "    loo_cols = [c for c in df.columns if c.startswith(\"_leftcirc_\")]\n",
    "    # mean abs rank drift per fold (ignore the held-out NaN)\n",
    "    drifts = {c: (df[c] - df[base]).abs().mean(skipna=True) for c in loo_cols}\n",
    "\n",
    "    # optional: Kendall-tau vs base per fold\n",
    "    taus = {}\n",
    "    base_order = df[base].dropna()\n",
    "    for c in loo_cols:\n",
    "        both = df[[base, c]].dropna()\n",
    "        taus[c] = kendalltau(both[base], both[c]).correlation\n",
    "    print(\"-\" * 30)\n",
    "    print(task)\n",
    "\n",
    "    print(\"Mean LOO mean-rank-drift:\", np.mean(list(drifts.values())))\n",
    "    print(\"Worst LOO mean-rank-drift:\", max(drifts.items(), key=lambda x: x[1]))\n",
    "    print(\"Mean Kendall-τ across LOO:\", np.nanmean(list(taus.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "for task, df in {\"lcb\": df_lcb, \"cxg\": df_c2t}.items():\n",
    "    base = \"OTER_rank\"\n",
    "    for i in range(len(df)):\n",
    "        new_df = df.drop(df.index[i]).copy()     # safer than concat slices\n",
    "        normalize(new_df)                         # must operate on new_df only\n",
    "        X_clean = remove_outliers(new_df)\n",
    "        all_possible_derivates = create_all_possible_derivatives(new_df[\"ene_eff\"], new_df[\"perf\"])\n",
    "        deriv_inliers_all = remove_derivative_outliers(all_possible_derivates)\n",
    "        coefficients, degree = approximate_regression_function(new_df, X_clean, deriv_inliers_all)\n",
    "        oter_rank = regression_rank(new_df, coefficients, degree)\n",
    "\n",
    "        col = f\"left_{i}\"\n",
    "        df[col] = np.nan\n",
    "        df.loc[oter_rank.index, col] = oter_rank\n",
    "\n",
    "    loo_cols = [c for c in df.columns if c.startswith(\"left_\")]\n",
    "    # mean abs rank drift per fold (ignore the held-out NaN)\n",
    "    drifts = {c: (df[c] - df[base]).abs().mean(skipna=True) for c in loo_cols}\n",
    "\n",
    "    # optional: Kendall-tau vs base per fold\n",
    "    taus = {}\n",
    "    base_order = df[base].dropna()\n",
    "    for c in loo_cols:\n",
    "        both = df[[base, c]].dropna()\n",
    "        taus[c] = kendalltau(both[base], both[c]).correlation\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(task)\n",
    "    print(\"Mean LOO mean-rank-drift:\", np.mean(list(drifts.values())))\n",
    "    print(\"Worst LOO mean-rank-drift:\", max(drifts.items(), key=lambda x: x[1]))\n",
    "    print(\"Mean Kendall-τ across LOO:\", np.nanmean(list(taus.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71826afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b16f1ab",
   "metadata": {},
   "source": [
    "## Further Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# === Edit these once ===\n",
    "DF = df_c2t          # or df_c2t (run the suite twice if you want)\n",
    "X_COL = \"perf\"            # your objective-1 column in [0,1]\n",
    "Y_COL = \"ene_eff\"            # your objective-2 column in [0,1]\n",
    "\n",
    "# Optional: if your rating isn’t in DF yet, provide a callable:\n",
    "def rate_fn_OTER(new_df):\n",
    "    X_clean = remove_outliers(new_df)\n",
    "    all_possible_derivates = create_all_possible_derivatives(new_df[\"ene_eff\"], new_df[\"perf\"])\n",
    "    deriv_inliers_all = remove_derivative_outliers(all_possible_derivates)\n",
    "    coefficients, degree = approximate_regression_function(new_df, X_clean, deriv_inliers_all)\n",
    "    oter_rank = regression_rank(new_df, coefficients, degree)\n",
    "    return oter_rank\n",
    "\n",
    "def rate_fn_CIRC(new_df):\n",
    "    dist = calculate_euc_formula(new_df)\n",
    "    circ_ranks = fill_distance_ranking(new_df, dist)\n",
    "    return circ_ranks\n",
    "\n",
    "def get_base_rating(df, method=\"circ\"):\n",
    "    return pd.Series(rate_fn_OTER(df) if method == \"oter\" else rate_fn_CIRC(df), index=df.index).astype(int)\n",
    "\n",
    "R_BASE = get_base_rating(DF)\n",
    "RANK_BASE = R_BASE.rank(method=\"average\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_robustness(df, eps=0.03, trials=20, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    r0 = get_base_rating(df)\n",
    "    changes = []\n",
    "    worst = 0\n",
    "    for _ in range(trials):\n",
    "        dx = rng.uniform(-eps, eps, len(df))\n",
    "        dy = rng.uniform(-eps, eps, len(df))\n",
    "        dfp = df.copy()\n",
    "        dfp[X_COL] = np.clip(df[X_COL].values + dx, 0, 1)\n",
    "        dfp[Y_COL] = np.clip(df[Y_COL].values + dy, 0, 1)\n",
    "        r1 = get_base_rating(dfp)\n",
    "        diff = (r1 - r0).abs()\n",
    "        changes.append(diff.mean())\n",
    "        worst = max(worst, diff.max())\n",
    "    return float(np.mean(changes)), int(worst)\n",
    "\n",
    "mean_abs, worst = noise_robustness(DF, eps=0.03, trials=20)\n",
    "print(f\"Mean |Δrating| @ ε=0.05: {mean_abs:.3f}  |  Worst-case Δrating: {worst}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0da49",
   "metadata": {},
   "source": [
    "## OTER different hyperaparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [3,4,5,6,7]\n",
    "MCD_percentile = [0.90, 0.95, 0.975]\n",
    "LES_quantile = [65, 70, 75, 80]\n",
    "intercept_at_one = [1e-1, 1e-2, 1e-3]\n",
    "# degrees = [1, 35]\n",
    "# MCD_percentile = [0.95]\n",
    "# LES_quantile = [75] # -inf ..... 0\n",
    "# intercept_at_one = [1e-2]\n",
    "perm_cols = []\n",
    "for df in [df_lcb, df_c2t]:\n",
    "    for deg in degrees:\n",
    "        for mcd_p in MCD_percentile:\n",
    "            for lq in LES_quantile:\n",
    "                for icpt in intercept_at_one:\n",
    "                    X_clean = remove_outliers(df, mcd_p)\n",
    "                    all_possible_derivates = create_all_possible_derivatives(df[\"ene_eff\"], df[\"perf\"])\n",
    "                    deriv_inliers_all = remove_derivative_outliers(all_possible_derivates)\n",
    "                    coefficients, degree = approximate_regression_function(df, X_clean, deriv_inliers_all, lq, deg, icpt)\n",
    "                    oter_rank = regression_rank(df, coefficients, deg)\n",
    "                    col_name = f\"deg_{deg}_mcd_{mcd_p}_ls_{lq}_icpt_{icpt}\" \n",
    "                    perm_cols.append(col_name)\n",
    "                    df[col_name] = oter_rank\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b03ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc474ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    " \n",
    "class OTERSensitivityAnalysis:\n",
    "    def __init__(self, df_lcb, df_c2t, baseline_config=(5, 0.95, 75, 0.01)):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_lcb, df_c2t : pd.DataFrame\n",
    "            Dataframes with OTER rank columns\n",
    "        baseline_config : tuple\n",
    "            (degree, mcd_percentile, les_quantile, intercept)\n",
    "        \"\"\"\n",
    "        self.df_lcb = df_lcb\n",
    "        self.df_c2t = df_c2t\n",
    "        # Construct baseline column name\n",
    "        deg, mcd, lq, icpt = baseline_config\n",
    "        # self.baseline_col = f\"deg_{deg}_mcd_{mcd}_ls_{lq}_icpt_{icpt}\"\n",
    "        self.baseline_col = f\"OTER_rank\"\n",
    "        # Get all OTER rank columns\n",
    "        self.rank_cols_lcb = [col for col in df_lcb.columns if col.startswith('deg_')]\n",
    "        self.rank_cols_c2t = [col for col in df_c2t.columns if col.startswith('deg_')]\n",
    "    def _parse_config(self, col_name):\n",
    "        \"\"\"Extract config from column name: deg_5_mcd_0.95_ls_75_icpt_0.01\"\"\"\n",
    "        parts = col_name.split('_')\n",
    "        return {\n",
    "            'degree': int(parts[1]),\n",
    "            'mcd': float(parts[3]),\n",
    "            'les': int(parts[5]),\n",
    "            'intercept': float(parts[7])\n",
    "        }\n",
    "    def compute_correlations(self, df, rank_cols, task_name):\n",
    "        \"\"\"Compute pairwise correlations for one task\"\"\"\n",
    "        baseline_ratings = df[self.baseline_col].values\n",
    "        results = []\n",
    "        for col in rank_cols:\n",
    "            if col == self.baseline_col:\n",
    "                continue\n",
    "            config = self._parse_config(col)\n",
    "            test_ratings = df[col].values\n",
    "            # Spearman correlation\n",
    "            rho, p_val = spearmanr(baseline_ratings, test_ratings)\n",
    "            # Rating changes\n",
    "            changes = np.abs(baseline_ratings - test_ratings)\n",
    "            results.append({\n",
    "                'task': task_name,\n",
    "                'config_name': col,\n",
    "                'degree': config['degree'],\n",
    "                'mcd': config['mcd'],\n",
    "                'les': config['les'],\n",
    "                'intercept': config['intercept'],\n",
    "                'spearman_rho': rho,\n",
    "                'p_value': p_val,\n",
    "                'mean_abs_change': changes.mean(),\n",
    "                'pct_no_change': (changes < 1).mean() * 100,  # Changed from <= 1 to < 1\n",
    "                'pct_change_0': (changes == 0).mean() * 100,  # Explicitly == 0\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "    def analyze_all(self):\n",
    "        \"\"\"Analyze both tasks\"\"\"\n",
    "        # Compute for both tasks\n",
    "        results_lcb = self.compute_correlations(self.df_lcb, self.rank_cols_lcb, 'LCB')\n",
    "        results_c2t = self.compute_correlations(self.df_c2t, self.rank_cols_c2t, 'CXG')\n",
    "        # Combine\n",
    "        all_results = pd.concat([results_lcb, results_c2t], ignore_index=True)\n",
    "        return all_results\n",
    "    def analyze_by_parameter(self, results_df, baseline_config=(5, 0.95, 75, 0.01)):\n",
    "        \"\"\"Analyze sensitivity for each parameter independently\"\"\"\n",
    "        deg_base, mcd_base, les_base, icpt_base = baseline_config\n",
    "        analysis = {}\n",
    "        # Degree analysis\n",
    "        degree_mask = (\n",
    "            (results_df['mcd'] == mcd_base) & \n",
    "            (results_df['les'] == les_base) & \n",
    "            (results_df['intercept'] == icpt_base)\n",
    "        )\n",
    "        analysis['degree'] = results_df[degree_mask].groupby('degree').agg({\n",
    "            'spearman_rho': ['mean', 'min', 'max'],\n",
    "            'p_value': ['mean', 'max'],\n",
    "            'pct_no_change': ['mean', 'min']  # Changed from pct_change_leq_1\n",
    "        }).round(4)\n",
    "        # MCD analysis\n",
    "        mcd_mask = (\n",
    "            (results_df['degree'] == deg_base) & \n",
    "            (results_df['les'] == les_base) & \n",
    "            (results_df['intercept'] == icpt_base)\n",
    "        )\n",
    "        analysis['mcd'] = results_df[mcd_mask].groupby('mcd').agg({\n",
    "            'spearman_rho': ['mean', 'min', 'max'],\n",
    "            'p_value': ['mean', 'max'],\n",
    "            'pct_no_change': ['mean', 'min']  # Changed\n",
    "        }).round(4)\n",
    "        # LES analysis\n",
    "        les_mask = (\n",
    "            (results_df['degree'] == deg_base) & \n",
    "            (results_df['mcd'] == mcd_base) & \n",
    "            (results_df['intercept'] == icpt_base)\n",
    "        )\n",
    "        analysis['les'] = results_df[les_mask].groupby('les').agg({\n",
    "            'spearman_rho': ['mean', 'min', 'max'],\n",
    "            'p_value': ['mean', 'max'],\n",
    "            'pct_no_change': ['mean', 'min']  # Changed\n",
    "        }).round(4)\n",
    "        # Intercept analysis\n",
    "        icpt_mask = (\n",
    "            (results_df['degree'] == deg_base) & \n",
    "            (results_df['mcd'] == mcd_base) & \n",
    "            (results_df['les'] == les_base)\n",
    "        )\n",
    "        analysis['intercept'] = results_df[icpt_mask].groupby('intercept').agg({\n",
    "            'spearman_rho': ['mean', 'min', 'max'],\n",
    "            'p_value': ['mean', 'max'],\n",
    "            'pct_no_change': ['mean', 'min']  # Changed\n",
    "        }).round(4)\n",
    "        return analysis\n",
    "    def compute_summary_statistics(self, results_df):\n",
    "        \"\"\"Overall statistics across all configurations\"\"\"\n",
    "        stats = {\n",
    "            'total_configs': len(results_df),\n",
    "            'mean_rho': results_df['spearman_rho'].mean(),\n",
    "            'min_rho': results_df['spearman_rho'].min(),\n",
    "            'max_rho': results_df['spearman_rho'].max(),\n",
    "            'std_rho': results_df['spearman_rho'].std(),\n",
    "            'configs_rho_above_085': (results_df['spearman_rho'] > 0.85).sum(),\n",
    "            'configs_rho_above_080': (results_df['spearman_rho'] > 0.80).sum(),\n",
    "            'mean_stability': results_df['pct_no_change'].mean(),  # Changed\n",
    "            'min_stability': results_df['pct_no_change'].min(),    # Changed\n",
    "            'configs_stability_above_80': (results_df['pct_no_change'] > 80).sum(),  # Changed\n",
    "            # Statistical significance metrics\n",
    "            'configs_significant_p005': (results_df['p_value'] < 0.05).sum(),\n",
    "            'configs_significant_p001': (results_df['p_value'] < 0.01).sum(),\n",
    "            'configs_significant_p0001': (results_df['p_value'] < 0.001).sum(),\n",
    "            'mean_p_value': results_df['p_value'].mean(),\n",
    "            'max_p_value': results_df['p_value'].max(),\n",
    "        }\n",
    "        # Per-task breakdown\n",
    "        for task in ['LCB', 'CXG']:\n",
    "            task_df = results_df[results_df['task'] == task]\n",
    "            stats[f'{task}_mean_rho'] = task_df['spearman_rho'].mean()\n",
    "            stats[f'{task}_min_rho'] = task_df['spearman_rho'].min()\n",
    "            stats[f'{task}_mean_stability'] = task_df['pct_no_change'].mean()  # Changed\n",
    "            stats[f'{task}_significant_p001'] = (task_df['p_value'] < 0.01).sum()\n",
    "        return stats\n",
    "    \n",
    "    def find_most_unstable_config_pairs(self, df, rank_cols, top_n=5):\n",
    "        \"\"\"\n",
    "        Find config pairs with highest % ≤1 class change difference\n",
    "        Returns: DataFrame with pairs and models that changed between them\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "        baseline_ratings = df[self.baseline_col].values\n",
    "        results = []\n",
    "        for col1, col2 in combinations(rank_cols, 2):\n",
    "            ratings1 = df[col1].values\n",
    "            ratings2 = df[col2].values\n",
    "            # Stability vs baseline\n",
    "            stab1 = (np.abs(ratings1 - baseline_ratings) <= 1).mean() * 100\n",
    "            stab2 = (np.abs(ratings2 - baseline_ratings) <= 1).mean() * 100\n",
    "            # Changes between configs\n",
    "            changes = np.abs(ratings1 - ratings2)\n",
    "            changed_models = df.loc[changes > 0, 'model'].tolist()\n",
    "            results.append({\n",
    "                'config1': col1,\n",
    "                'config2': col2,\n",
    "                'stability_diff': abs(stab1 - stab2),\n",
    "                'models_changed': len(changed_models),\n",
    "                'changed_model_ids': ', '.join(changed_models),\n",
    "                'max_rating_change': changes.max()\n",
    "            })\n",
    "        return pd.DataFrame(results).sort_values('stability_diff', ascending=False).head(top_n)\n",
    "    \n",
    "    def print_summary(self, results_df, param_analysis, stats):\n",
    "        \"\"\"Print formatted summary\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"OTER PARAMETER SENSITIVITY ANALYSIS - SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n{'OVERALL STATISTICS':-^70}\")\n",
    "        print(f\"Total configurations tested: {stats['total_configs']}\")\n",
    "        print(f\"Baseline: {self.baseline_col}\")\n",
    "        print(f\"\\n{'Spearman Correlation:':-^70}\")\n",
    "        print(f\"  Mean ρ:  {stats['mean_rho']:.3f}\")\n",
    "        print(f\"  Min ρ:   {stats['min_rho']:.3f}\")\n",
    "        print(f\"  Max ρ:   {stats['max_rho']:.3f}\")\n",
    "        print(f\"  Std ρ:   {stats['std_rho']:.3f}\")\n",
    "        print(f\"  Configs with ρ > 0.85: {stats['configs_rho_above_085']}/{stats['total_configs']} ({stats['configs_rho_above_085']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"  Configs with ρ > 0.80: {stats['configs_rho_above_080']}/{stats['total_configs']} ({stats['configs_rho_above_080']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"\\n{'Statistical Significance:':-^70}\")\n",
    "        print(f\"  Configs with p < 0.05:   {stats['configs_significant_p005']}/{stats['total_configs']} ({stats['configs_significant_p005']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"  Configs with p < 0.01:   {stats['configs_significant_p001']}/{stats['total_configs']} ({stats['configs_significant_p001']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"  Configs with p < 0.001:  {stats['configs_significant_p0001']}/{stats['total_configs']} ({stats['configs_significant_p0001']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"  Mean p-value: {stats['mean_p_value']:.6f}\")\n",
    "        print(f\"  Max p-value:  {stats['max_p_value']:.6f}\")\n",
    "        print(f\"\\n{'Rating Stability:':-^70}\")\n",
    "        print(f\"  Mean % no change: {stats['mean_stability']:.1f}%\")  # Changed label\n",
    "        print(f\"  Min % no change:  {stats['min_stability']:.1f}%\")   # Changed label\n",
    "        print(f\"  Configs with >80% stability: {stats['configs_stability_above_80']}/{stats['total_configs']} ({stats['configs_stability_above_80']/stats['total_configs']*100:.1f}%)\")\n",
    "        print(f\"\\n{'Per-Task Breakdown:':-^70}\")\n",
    "        print(f\"  LCB - Mean ρ: {stats['LCB_mean_rho']:.3f}, Min ρ: {stats['LCB_min_rho']:.3f}, Stability: {stats['LCB_mean_stability']:.1f}%, p<0.01: {stats['LCB_significant_p001']}\")\n",
    "        print(f\"  CXG - Mean ρ: {stats['CXG_mean_rho']:.3f}, Min ρ: {stats['CXG_min_rho']:.3f}, Stability: {stats['CXG_mean_stability']:.1f}%, p<0.01: {stats['CXG_significant_p001']}\")\n",
    "        print(f\"\\n{'PARAMETER-SPECIFIC SENSITIVITY':-^70}\")\n",
    "        for param_name, param_df in param_analysis.items():\n",
    "            print(f\"\\n{param_name.upper()}:\")\n",
    "            print(param_df)\n",
    " \n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    " \n",
    "# Initialize\n",
    "analyzer = OTERSensitivityAnalysis(\n",
    "    df_lcb, \n",
    "    df_c2t, \n",
    "    baseline_config=(5, 0.95, 75, 0.01)  # deg, mcd, les, intercept\n",
    ")\n",
    " \n",
    "# Run analysis\n",
    "print(\"Computing correlations...\")\n",
    "all_results = analyzer.analyze_all()\n",
    " \n",
    "print(\"Analyzing by parameter...\")\n",
    "param_analysis = analyzer.analyze_by_parameter(all_results)\n",
    " \n",
    "print(\"Computing summary statistics...\")\n",
    "stats = analyzer.compute_summary_statistics(all_results)\n",
    " \n",
    "# Print summary\n",
    "analyzer.print_summary(all_results, param_analysis, stats)\n",
    " \n",
    "# Export results\n",
    "all_results.to_csv('oter_sensitivity_results.csv', index=False)\n",
    "print(f\"\\n✓ Detailed results saved to 'oter_sensitivity_results.csv'\")\n",
    " \n",
    "# Summary statistics to DataFrame for easy viewing\n",
    "summary_df = pd.DataFrame([stats]).T\n",
    "summary_df.columns = ['Value']\n",
    "summary_df.to_csv('oter_sensitivity_summary.csv')\n",
    "print(f\"✓ Summary statistics saved to 'oter_sensitivity_summary.csv'\")\n",
    " \n",
    "# Parameter analysis to Excel (multiple sheets)\n",
    "with pd.ExcelWriter('oter_parameter_analysis.xlsx') as writer:\n",
    "    for param_name, param_df in param_analysis.items():\n",
    "        param_df.to_excel(writer, sheet_name=param_name)\n",
    "print(f\"✓ Parameter analysis saved to 'oter_parameter_analysis.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79b184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b1b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sus_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
